
基于GPU开发板的聊天机器 (Chinese ChatBot on NVIDIA TX1)
=========================================================

(Latest Update: 2017/06/30)

语音助手能够通过智能对话与即时问答的智能交互，实现帮忙用户解决问题，其主要是帮忙用户解决生活类问题。近年来神经网络的兴起大大提高了语音识别的准确性，从而提高了语音助手的性能，但是其计算量之大对移动端产品造成了极大负担。本设计中，我们借助GPU开发板，在NVIDIA TX1平台上搭建了一个基于神经网络的语音识别助手原型。最终系统包括带抗噪预处理的语音输入、本地基于神经网络的语音识别、本地语义处理、云端问答和本地应答发声模块。此外，为了可视化展示系统，我们还辅助搭建了基于Web的可视化交互。


工作背景与相关工作
==================

传统语音识别基本流程
--------------------

传统的语音识别主要包括以下几个步骤:

1.  输入变换：对于以16000波特率采集的音频数据进行特征提取和预处理变换，包括原始波形切片、频谱特和MFCC特征等等。

2.  声学模型。声学模型用于表示音频信号和输入中其他言语单元的关系。声学模型一般通过数据库中的录音和相应翻译内容进行识别。

3.  语言模型。基于统计的语言模型是关于一串字符的概率分布。给定一串m长的序列，它能够对整个句子分配一个概率$P(W_{1},\ldots,W_{m})$。这些概率通过训练集中的样本学习得到。

利用神经网络进行语音识别
------------------------

语音识别方面的经典算法包括HMM 声学模型、N-Gram 语言模型和CTC算法等等，都是比较传统的方法并取得了很好的效果。近年来深度学习的发展也让语音识别的效果更上一个台阶。为了充分提高识别的效果并增大GPU开发板的使用率，本项目采取基于深度学习的方法进行识别。我们基于百度的DeepSpeech 框架进行实现。


Deep Speech框架的神经网络由卷积层（Convolution）、双向循环层（BiRNN）和全连接层（FC）组成。对于卷积层的输入，我们采用原始的频谱特征向量作为输入。经过试验发现三层卷积层具有不错的识别效果，在后续实验中将保持三层卷积的结构。 BiRNN层特别适用于处理语音信号，因为该层使得网络既可以访问过去的上下文，又可以访问将来的上下文。 BiRNN的输出后接一层全连接层，每一个单元对应一个可能的词汇。比如我们使用的中文语料库，里包含大概3645个常见词汇，还有空白字符作为未知token，以使CTC能够有效处理连续重复符号。

自然语言语义分析
----------------

任何对语言的理解都可以归为语义分析的范畴。一段文本通常由词、句子和段落来构成，根据理解对象的语言单位不同，语义分析又可进一步分解为词汇级语义分析、句子级语义分析以及篇章级语义分析。

语义分析的目标就是通过建立有效的模型和系统， 实现在各个语言单位（包括词汇、句子和篇章等）的自动语义分析，从而实现理解整个文本表达的真实语义。

智能语音交互与微软小冰
----------------------

“微软小冰”是微软亚洲互联网工程院在2014年5月29日发布一款人工智能聊天伴侣虚拟机器人。 考虑到本地处理语音回答资源不足，我们引入线上回答机制，将本地识别出的问题经过文本预处理后发送给“微软小冰”，利用其强大的文本理解特性对本地处理不了的问题进行回答，我们即可获得回答的文本。

GPU开发板NVIDIA TX1
-------------------

英伟达(NVIDIA)的GPU开发板TX1套件是一个低功耗、移动高端嵌入式GPU开发板。它具有四核Cortex A57的CPU核，主频高达2.2GHz。同时TX1配备又NVIDIA GeForce Maxwell移动图像处理单元，将CUDA计算拓展至开发板，具有256个CUDA核心。

基于GPU开发板的ChatBot系统架构
============================

我们设计的ChatBot的架构如图3所示。

整体架构
--------

如图3所示，我们的ChatBot首先通过麦克风进行语音采集，在CPU上对采集的语音进行切割、降噪等预处理，然后转化为频谱图，传送给GPU上，通过GPU上的神经网络对语音频谱图进行识别，将识别文本传送回CPU，进行根据语音的预处理，然后再发送至云端，使用微软小冰API进行智能回复，从云端获取到回复的文本后，我们在本地将文本转化呈语音，通过音响进行发声应答。

语音采集
--------

基于GPU开发板的ChatBot的输入为使用者的语音信号，系统的语音采集模块对输入的数据进行检测，判别输入信号是否为语音信号，并存储语音信号用于后续预处理步骤。

信号时域分析中，能量及过零率可以作为信号的特征对不同类型的信号进行区分：

a) t时刻信号能量：

$$E\left( t \right) = \sum_{n = 0}^{N - 1}\left\lbrack s\left( t + n \right) \right\rbrack^{2}$$

b) t时刻过零率：

$$Z\left( t \right) = \frac{1}{2}\{\sum_{n = 0}^{N - 1}|\text{sgn}\left\lbrack s\left( t + n \right) \right\rbrack - \text{sgn}\lbrack s(t + n - 1)\rbrack|\}$$

与噪声相比，人类发音具有特定的特点，可以根据音节发音的特点，对语音信号的特征做如下分析：

1) 清音段：能量低，过零率高，波形特点类似随机的噪声，该部分信号与语音的辅音段对应；

2) 浊音段：能量高，过零率低，波形具有周期性特点。

本项目中信号采样率为16kHz，对采集信号进行分帧处理，每帧包括1024个采样点（对应帧长0.064s），分析每一帧信号的能量及过零率，从而判断是否为语音信号，将输入的语音信号进行存储，非语音信号则被作为噪声信号进行存储。

语音预处理
----------

对于采集到的语音信号，需要进行降噪及分割两步预处理，降噪用于去除背景噪声，提高语音信号识别的正确率，分割操作将语音信号按字进行分割，用于后续识别语音中的每一个字。

1.  降噪：分别对采集到的语音信号及噪声信号做FFT变换，从语音信号的频谱中减去噪声频谱，在进行IFFT变换，获得降噪后带背景抑制的音频信号。

2.  分割：根据信号能量进行分割，低于一定能量阈值的信号视为间隔，在实现过程中，我们设定一句话的最短时长为1s，语句间的最短间隔为0.5s，小于语句间最短间隔的视为语句中每个字之间的间隔。

语音识别
--------

语音识别部分，我们采用DeepSpeech神经网络识别。首先，我们把采集到的语音转化为频谱图作为神经网络的输入，然后通过3层CNN、7层GRU、1层FC获得识别的文本。

神经网络的训练我们使用了开源的thchs30数据集和2008年的新闻联播数据集。我们首先将数据集按照文本切割成每个语音只有一句话，进行神经网络的训练。我们一共训练了20轮，最后在训练数据集的测试集上汉字错误率达到了23%的水平。

我们将神经网络的前向语音识别部署在GPU开发板的GPU部分。因为GPU运算核心数远大于CPU，十分适合并行进行矩阵乘法加速。同时，由于GPU的显存大，适合循环神经网络展开，提高了访存速度。因此在GPU开发板上的GPU上运行语音识别的神经网络，具有加速和降低功耗量两方面的好处。

语义处理
--------

语义分析的研究尚且处在一个蓬勃发展的时期，鲁棒地做到对用户意义的正确理解还比较困难。为此我们先做基于关键词的句子理解，通过现有的知识库，对语音助手使用过程中可能频繁使用到的交互命令(如天气、驾驶路线等常用询问)建模并索引，以通过可能句式混乱的源端句子得到能够被云端正确处理的目标端句子。

由于没有使用商业级的训练数据，我们训练得到的神经网络输出有比较好的音节识别准确率，但在汉字选用上经常会出现一些问题。为此，为了提高语音助手的鲁棒性，我们使用pypinyin库先从汉字转到拼音源，再进行匹配理解处理，这个操作在目前框架下可以大大提高语义理解系统的鲁棒性。

智能回答
--------

利用微软小冰提供的API——itchat，将语音文本发送给小冰，并收到回答。编写程序时，控制收到的回答仅有文本有效，如果收到的回答是非文本内容，则重发问题，直到收到文本为止。

处理时，需要解决的另一个问题是多线程通信。本系统中，存在几个典型的生产者-消费者问题——语音识别模块生产问题，回答模块消费问题，并生产出回答；发音模块消费回答。在没有生产资料的时候是不能消费的，因此解决各个线程之间的通信是一个重要问题。

我们最终采取共享阻塞队列的方法解决这一问题。生产者和消费者彼此之间不直接通讯，而借由一个阻塞队列来进行通讯。所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。

应答发声Echo
------------

利用中文语音合成软件Ekho来实现本地发音。其原理包括三个核心步骤：

（1）文本分析: 对输入文本进行语言学分析（主要模拟人对自然语言的理解过程），逐句进行词汇的、语法的和语义的分析，以确定句子的低层结构和每个字的音素的组成，包括文本的断句、字词切分、多音字的处理、数字的处理、缩略语的处理等。使计算机对输入的文本能完全理解，并给出后两部分所需要的各种发音提示。

（2）韵律建模: 为合成语音规划出音段特征，如音高、音长和音强等，使合成语音能正确表达语意，听起来更加自然。

（3）语音合成（核心模块）: 根据韵律建模的结果，把处理好的文本所对应的单字或短语的语音基元从语音合成库中提取，利用特定的语音合成技术对语音基元进行韵律特性的调整和修改，最终合成出符合要求的语音。

由此将回答文本转变为可以听得懂的、流利的汉语口语通过麦克风输出。
